{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 327,
     "status": "ok",
     "timestamp": 1764870410171,
     "user": {
      "displayName": "Аrtem Lampitskyi",
      "userId": "08318838441185537497"
     },
     "user_tz": -120
    },
    "id": "D0Oo4Wi3dLtW",
    "outputId": "d1d36eff-42ca-4d57-cbab-56e0bc37dc89"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!rm -f /content/main.zip\n",
    "!wget https://raw.githubusercontent.com/Ostap-Kucher/Labs/main/main.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 111,
     "status": "ok",
     "timestamp": 1764870411520,
     "user": {
      "displayName": "Аrtem Lampitskyi",
      "userId": "08318838441185537497"
     },
     "user_tz": -120
    },
    "id": "D8u9NwX5dgnT",
    "outputId": "5128544d-9003-4b15-f3ef-ca86b8bcd874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/main.zip\n",
      "   creating: lab1-main/\n",
      "  inflating: __MACOSX/._lab1-main    \n",
      "  inflating: lab1-main/.DS_Store     \n",
      "  inflating: __MACOSX/lab1-main/._.DS_Store  \n",
      "  inflating: lab1-main/eng texts.txt  \n",
      "  inflating: __MACOSX/lab1-main/._eng texts.txt  \n",
      "  inflating: lab1-main/ukr texts.txt  \n",
      "  inflating: __MACOSX/lab1-main/._ukr texts.txt  \n",
      "  inflating: lab1-main/README.md     \n",
      "  inflating: __MACOSX/lab1-main/._README.md  \n"
     ]
    }
   ],
   "source": [
    "!unzip /content/main.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 67441,
     "status": "ok",
     "timestamp": 1764870482850,
     "user": {
      "displayName": "Аrtem Lampitskyi",
      "userId": "08318838441185537497"
     },
     "user_tz": -120
    },
    "id": "pPPmeQBwdlz-",
    "outputId": "e7124dff-d6f4-4245-b094-a165dd129d4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n",
      "Приклад до очищення: Sleep is a fundamental biological necessity, not a luxury we can easily sacrifice. During the night, your brain remains incredibly active, consolidating memories and processing the day's information. Getting adequate rest is crucial for cognitive function, directly affecting your ability to focus, solve problems, and make decisions. Scientists recommend that most adults aim for seven to nine hours of quality sleep per night. Chronic lack of sleep, or sleep deprivation, can lead to serious health issues, including a weakened immune system. It also increases the risk of developing conditions like diabetes, heart disease, and high blood pressure. Furthermore, poor sleep significantly impacts mood, often leading to irritability, stress, and even depression. To improve sleep hygiene, establish a consistent bedtime routine, even on weekends. Avoid consuming caffeine and large meals close to bedtime, as they can interfere with your natural sleep cycle. The bedroom environment should be dark, quiet, and cool to promote deep rest. Limiting screen time before bed is also vital, as the blue light from devices suppresses the production of melatonin, the sleep hormone. A short nap during the day can sometimes be helpful, but it should not be too long or too close to the evening. Quality sleep is directly linked to better physical recovery after exercise. It plays a key role in muscle repair and growth. In conclusion, viewing sleep as an investment in your physical and mental health is the best way to ensure long-term well-being. Prioritizing sleep will inevitably lead to a more productive and happier life.\n",
      "Приклад після очищення: sleep is a fundamental biological necessity, not a luxury we can easily sacrifice. during the night, your brain remains incredibly active, consolidating memories and processing the day's information. getting adequate rest is crucial for cognitive function, directly affecting your ability to focus, solve problems, and make decisions. scientists recommend that most adults aim for seven to nine hours of quality sleep per night. chronic lack of sleep, or sleep deprivation, can lead to serious health issues, including a weakened immune system. it also increases the risk of developing conditions like diabetes, heart disease, and high blood pressure. furthermore, poor sleep significantly impacts mood, often leading to irritability, stress, and even depression. to improve sleep hygiene, establish a consistent bedtime routine, even on weekends. avoid consuming caffeine and large meals close to bedtime, as they can interfere with your natural sleep cycle. the bedroom environment should be dark, quiet, and cool to promote deep rest. limiting screen time before bed is also vital, as the blue light from devices suppresses the production of melatonin, the sleep hormone. a short nap during the day can sometimes be helpful, but it should not be too long or too close to the evening. quality sleep is directly linked to better physical recovery after exercise. it plays a key role in muscle repair and growth. in conclusion, viewing sleep as an investment in your physical and mental health is the best way to ensure long-term well-being. prioritizing sleep will inevitably lead to a more productive and happier life.\n",
      "Обсяг символів у корпусі: 16645\n",
      "Кількість речень: 35\n",
      "Розмір словника: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">50,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">60,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">101,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m50\u001b[0m)          │        \u001b[38;5;34m50,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m60,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)           │       \u001b[38;5;34m101,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">211,400</span> (825.78 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m211,400\u001b[0m (825.78 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">211,400</span> (825.78 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m211,400\u001b[0m (825.78 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.1704 - loss: 6.4354\n",
      "Epoch 2/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.1872 - loss: 5.4881\n",
      "Epoch 3/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2008 - loss: 5.2576\n",
      "Epoch 4/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.1909 - loss: 5.1757\n",
      "Epoch 5/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.2039 - loss: 4.9335\n",
      "Epoch 6/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.1996 - loss: 4.9059\n",
      "Epoch 7/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.2020 - loss: 4.7455\n",
      "Epoch 8/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2136 - loss: 4.6261\n",
      "Epoch 9/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2200 - loss: 4.5065\n",
      "Epoch 10/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2190 - loss: 4.4095\n",
      "Epoch 11/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2307 - loss: 4.3057\n",
      "Epoch 12/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.2475 - loss: 4.1531\n",
      "Epoch 13/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2262 - loss: 4.1038\n",
      "Epoch 14/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.2565 - loss: 3.9691\n",
      "Epoch 15/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.2433 - loss: 3.9138\n",
      "Epoch 16/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.2582 - loss: 3.7994\n",
      "Epoch 17/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2674 - loss: 3.7226\n",
      "Epoch 18/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2665 - loss: 3.6664\n",
      "Epoch 19/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2690 - loss: 3.5878\n",
      "Epoch 20/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2834 - loss: 3.4854\n",
      "Epoch 21/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.2862 - loss: 3.4132\n",
      "Epoch 22/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3292 - loss: 3.2370\n",
      "Epoch 23/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3172 - loss: 3.1796\n",
      "Epoch 24/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.3261 - loss: 3.1061\n",
      "Epoch 25/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.3285 - loss: 3.0430\n",
      "Epoch 26/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3544 - loss: 2.9615\n",
      "Epoch 27/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3610 - loss: 2.8556\n",
      "Epoch 28/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4002 - loss: 2.8004\n",
      "Epoch 29/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.3868 - loss: 2.7613\n",
      "Epoch 30/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4220 - loss: 2.6247\n",
      "Epoch 31/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4562 - loss: 2.4935\n",
      "Epoch 32/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.4476 - loss: 2.5222\n",
      "Epoch 33/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.4895 - loss: 2.3762\n",
      "Epoch 34/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5189 - loss: 2.2779\n",
      "Epoch 35/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5245 - loss: 2.2853\n",
      "Epoch 36/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.5569 - loss: 2.1777\n",
      "Epoch 37/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5735 - loss: 2.1059\n",
      "Epoch 38/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.5683 - loss: 2.0935\n",
      "Epoch 39/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6146 - loss: 1.9868\n",
      "Epoch 40/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6201 - loss: 1.8736\n",
      "Epoch 41/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6487 - loss: 1.8450\n",
      "Epoch 42/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.6708 - loss: 1.7473\n",
      "Epoch 43/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6826 - loss: 1.7119\n",
      "Epoch 44/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7077 - loss: 1.6495\n",
      "Epoch 45/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7361 - loss: 1.5123\n",
      "Epoch 46/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.7456 - loss: 1.4782\n",
      "Epoch 47/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7481 - loss: 1.4666\n",
      "Epoch 48/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7840 - loss: 1.3648\n",
      "Epoch 49/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7950 - loss: 1.3057\n",
      "Epoch 50/50\n",
      "\u001b[1m145/145\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.7967 - loss: 1.2859\n",
      "Згенерований текст: this is a programme adenosine during the reef for data from it до of\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer  # імпортуємо токенізатор для перетворення тексту в числа\n",
    "from tensorflow.keras.utils import to_categorical, pad_sequences  # для перетворення міток та вирівнювання послідовностей\n",
    "from tensorflow.keras.models import Sequential  # для створення послідовної моделі нейронної мережі\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense  # імпортуємо шари: Embedding, LSTM, Dense\n",
    "import numpy as np  # імпортуємо numpy для роботи з масивами чисел\n",
    "\n",
    "# Перевірка версії TensorFlow\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# 2. Підключення файлів з текстами\n",
    "with open(\"/content/lab1-main/eng texts.txt\", \"r\", encoding=\"latin-1\") as f:\n",
    "    eng_texts = f.read().split(\"\\n\")  # читаємо всі рядки та розбиваємо на список рядків\n",
    "\n",
    "with open(\"/content/lab1-main/ukr texts.txt\", \"r\", encoding=\"mac_cyrillic\") as f:\n",
    "    ukr_texts = f.read().split(\"\\n\")\n",
    "\n",
    "# Об'єднуємо всі тексти для одного корпусу\n",
    "texts = eng_texts + ukr_texts\n",
    "\n",
    "# 3. Очищення тексту (базове: нижній регістр, видалення зайвих пробілів)\n",
    "cleaned_texts = [t.lower().strip() for t in texts]  # перетворюємо кожен рядок у нижній регістр і прибираємо зайві пробіли\n",
    "print(\"Приклад до очищення:\", texts[0])\n",
    "print(\"Приклад після очищення:\", cleaned_texts[0])\n",
    "\n",
    "# 4. Базова статистика\n",
    "total_chars = sum(len(t) for t in cleaned_texts)  # обчислюємо загальну кількість символів у всіх рядках\n",
    "total_sentences = len(cleaned_texts)  # обчислюємо кількість рядків (речень)\n",
    "print(\"Обсяг символів у корпусі:\", total_chars)\n",
    "print(\"Кількість речень:\", total_sentences)\n",
    "\n",
    "# 5. Токенізація (word-level)\n",
    "max_tokens = 1000  # максимальний розмір словника\n",
    "tokenizer = Tokenizer(num_words=max_tokens, oov_token=\"<OOV>\")  # створюємо токенізатор з обмеженням словника та OOV токеном для незнайомих слів\n",
    "tokenizer.fit_on_texts(cleaned_texts)  # навчаємо токенізатор на всіх рядках тексту\n",
    "sequences = tokenizer.texts_to_sequences(cleaned_texts)  # перетворюємо текст у послідовності чисел (кожне слово-число)\n",
    "word_index = tokenizer.word_index  # отримуємо словник: слово-номер\n",
    "print(\"Розмір словника:\", min(len(word_index), max_tokens))\n",
    "\n",
    "# 6. Формування зразків для моделі\n",
    "seq_len = 5  # довжина вхідного контексту (скільки слів беремо для передбачення наступного)\n",
    "X, y = [], []  # створюємо порожні списки для вхідних та вихідних даних\n",
    "for seq in sequences:  # проходимо по кожній послідовності слів\n",
    "    for i in range(seq_len, len(seq)):  # починаємо з позиції seq_len до кінця послідовності\n",
    "        X.append(seq[i-seq_len:i])  # беремо попередні seq_len слів як вхід\n",
    "        y.append(seq[i])  # беремо наступне слово як мітку\n",
    "X = np.array(X)  # перетворюємо список X у numpy масив\n",
    "y = to_categorical(y, num_classes=min(len(word_index)+1, max_tokens))\n",
    "\n",
    "# 7. Архітектура моделі\n",
    "vocab_size = min(len(word_index)+1, max_tokens)  # розмір словника для Embedding та Dense\n",
    "embedding_dim = 50  # розмірність векторного подання слова\n",
    "\n",
    "model = Sequential([  # створюємо послідовну модель\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=seq_len),  # шар Embedding: слова-вектори\n",
    "    LSTM(100),  # LSTM шар з 100 нейронами для обробки послідовностей\n",
    "    Dense(vocab_size, activation='softmax')  # вихідний Dense шар з активацією softmax на всі слова словника\n",
    "])\n",
    "model.build(input_shape=(None, seq_len))  # явно створюємо модель з заданою вхідною формою\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # компілюємо модель: крос-ентропія + Adam + точність\n",
    "model.summary()  # показуємо коротку інформацію про модель\n",
    "\n",
    "# 8. Навчання моделі\n",
    "model.fit(X, y, epochs=50, batch_size=16)\n",
    "\n",
    "# 9. Генерація тексту\n",
    "def generate_text(seed_text, next_words=10):  # функція для генерації тексту, seed_text — початковий рядок\n",
    "    result = seed_text.split()  # розбиваємо початковий рядок на список слів\n",
    "    for _ in range(next_words):  # повторюємо next_words разів\n",
    "        token_list = tokenizer.texts_to_sequences([\" \".join(result[-seq_len:])])[0]  # беремо останні seq_len слів та перетворюємо їх у числа\n",
    "        token_list = pad_sequences([token_list], maxlen=seq_len)  # вирівнюємо послідовність до довжини seq_len\n",
    "        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)[0]  # передбачаємо наступне слово (номер)\n",
    "        output_word = tokenizer.index_word.get(predicted, \"<OOV>\")  # отримуємо слово з номера або <OOV>\n",
    "        result.append(output_word)  # додаємо передбачене слово до результату\n",
    "    return \" \".join(result)  # об'єднуємо всі слова у рядок і повертаємо\n",
    "\n",
    "# Приклад генерації тексту англійською\n",
    "seed = \"this is a programme\"\n",
    "generated = generate_text(seed, next_words=10)  # генеруємо 10 слів після початкового рядка\n",
    "print(\"Згенерований текст:\", generated)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNWBBBIe++J0h72cEZoVITu",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
